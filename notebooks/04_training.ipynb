{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de03b7ed",
   "metadata": {
    "id": "de03b7ed"
   },
   "source": [
    "# Training\n",
    "\n",
    "In this notebook we will go through the basic process of training a an automatic mixing model. This will involve combining a dataset with a model and an appropriate training loop. For this demonstration we will [PyTorch Lightning](https://www.pytorchlightning.ai/) to faciliate the training. \n",
    "\n",
    "## Dataset\n",
    "For this demonstration we will use the subset of the [DSD100 dataset](https://sigsep.github.io/datasets/dsd100.html). This is a music source separation data, but we will use it to demonstrate how you can train a model. This is a very small subset of the dataset so it can easily be downloaded and we should not expect that our model will perform very well after training. \n",
    "\n",
    "This notebook can be used as a starting point for example by swapping out the dataset for a different dataset such as [ENST-drums](https://perso.telecom-paristech.fr/grichard/ENST-drums/) or [MedleyDB](https://medleydb.weebly.com/) after they have been downloaded. Since they are quite large, we will focus only on this small dataset for demonstration purposes. \n",
    "\n",
    "## GPU\n",
    "\n",
    "This notebook supports training with the GPU. You can achieve this by setting the `Runtime` to `GPU` in Colab using the menu bar at the top.\n",
    "\n",
    "## Learn More\n",
    "\n",
    "If you want to train these models on your own server and have much more control beyond this demo we encourage you to take a look at the training recipes we provide in the [automix-toolkit](https://github.com/csteinmetz1/automix-toolkit) repository.\n",
    "\n",
    "But, let's get started by installing the automix-toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb667664",
   "metadata": {
    "id": "eb667664"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/csteinmetz1/automix-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2ce50",
   "metadata": {
    "id": "a7b2ce50"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import IPython\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automix.data import DSD100Dataset\n",
    "from automix.system import System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60abda",
   "metadata": {
    "id": "ee60abda"
   },
   "source": [
    "First we will download the dataset subset and unzip the archive as well as the pretrained encoder checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac8885",
   "metadata": {
    "id": "d9ac8885",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints/\", exist_ok=True)\n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/encoder.ckpt\n",
    "!mv encoder.ckpt checkpoints/encoder.ckpt\n",
    "    \n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/DSD100subset.zip\n",
    "!unzip -o DSD100subset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa5f44",
   "metadata": {
    "id": "99fa5f44"
   },
   "source": [
    "# Configuration\n",
    "Here we select where we want to train on CPU or GPU and what model we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xZnS9suAVtvT",
   "metadata": {
    "id": "xZnS9suAVtvT"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi # check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c36228",
   "metadata": {
    "id": "25c36228"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_dir\" :  \"./DSD100subset\",\n",
    "    \"dataset_name\" : \"DSD100\",\n",
    "    \"automix_model\" : \"dmc\",\n",
    "    \"pretrained_encoder\" : True,\n",
    "    \"train_length\" : 65536,\n",
    "    \"val_length\" : 65536,\n",
    "    \"accelerator\" : \"gpu\", # you can select \"cpu\" or \"gpu\"\n",
    "    \"devices\" : 1, \n",
    "    \"batch_size\" : 4,\n",
    "    \"lr\" : 3e-4,\n",
    "    \"max_epochs\" : 10, \n",
    "    \"schedule\" : \"none\",\n",
    "    \"recon_losses\" : [\"sd\"],\n",
    "    \"recon_loss_weights\" : [1.0],\n",
    "    \"sample_rate\" : 44100,\n",
    "    \"num_workers\" : 2,\n",
    "}\n",
    "args = Namespace(**args)\n",
    "    \n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece37f55",
   "metadata": {
    "id": "ece37f55"
   },
   "outputs": [],
   "source": [
    "# setup callbacks\n",
    "callbacks = [\n",
    "    #LogAudioCallback(),\n",
    "    pl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "    pl.callbacks.ModelCheckpoint(\n",
    "        filename=f\"{args.dataset_name}-{args.automix_model}\"\n",
    "        + \"_epoch-{epoch}-step-{step}\",\n",
    "        monitor=\"val/loss_epoch\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        auto_insert_metric_name=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# we not will use weights and biases\n",
    "#wandb_logger = WandbLogger(save_dir=log_dir, project=\"automix-notebook\")\n",
    "\n",
    "# create PyTorch Lightning trainer\n",
    "# trainer = pl.Trainer(args, callbacks=callbacks)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=args.accelerator,\n",
    "    devices=args.devices,\n",
    "    callbacks=callbacks,\n",
    "    # Add other trainer arguments here if needed\n",
    ")\n",
    "\n",
    "# create the System\n",
    "system = System(**vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ac075",
   "metadata": {
    "id": "254ac075"
   },
   "source": [
    "# Dataset\n",
    "Now we will create datasets for train/val/test but we will use the same four songs across all sets here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f008ad9",
   "metadata": {
    "id": "4f008ad9"
   },
   "outputs": [],
   "source": [
    "train_dataset = DSD100Dataset(\n",
    "    args.dataset_dir,\n",
    "    args.train_length,\n",
    "    44100,\n",
    "    indices=[0, 4],\n",
    "    num_examples_per_epoch=100,\n",
    ")\n",
    "val_dataset = DSD100Dataset(\n",
    "    args.dataset_dir,\n",
    "    args.val_length,\n",
    "    44100,\n",
    "    indices=[0, 4],\n",
    "    num_examples_per_epoch=100,\n",
    ")\n",
    "test_dataset = DSD100Dataset(\n",
    "    args.dataset_dir,\n",
    "    args.train_length,\n",
    "    44100,\n",
    "    indices=[0, 4],\n",
    "    num_examples_per_epoch=100,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZsqvYgCLdY0U",
   "metadata": {
    "id": "ZsqvYgCLdY0U"
   },
   "source": [
    "# Logging\n",
    "We can launch an instance of TensorBoard within our notebook to monitor the training process. Be patient, it can take ~60 seconds for the window to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZOzuvl46WZuw",
   "metadata": {
    "id": "ZOzuvl46WZuw"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir=\"lightning_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6bb22",
   "metadata": {
    "id": "2ca6bb22"
   },
   "source": [
    "# Train!\n",
    "Now we are ready to launch the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5158c",
   "metadata": {
    "id": "19d5158c"
   },
   "outputs": [],
   "source": [
    "trainer.fit(system, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027cec5-b56d-448b-9369-7c2d761ed932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "jm9aXEMvY2Tc",
   "metadata": {
    "id": "jm9aXEMvY2Tc"
   },
   "source": [
    "# Test\n",
    "After training for a few epochs we will test the system by creating a mix from one of the songs that was in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pGusqW7MY0WY",
   "metadata": {
    "id": "pGusqW7MY0WY"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import torchaudio\n",
    "start_sample = 262144 * 2\n",
    "end_sample = 262144 * 3\n",
    "\n",
    "# load the input tracks\n",
    "track_dir = \"DSD100subset/Sources/Dev/081 - Patrick Talbot - Set Me Free/\"\n",
    "track_ext = \"wav\"\n",
    "\n",
    "track_filepaths = glob.glob(os.path.join(track_dir, f\"*.{track_ext}\"))\n",
    "track_filepaths = sorted(track_filepaths)\n",
    "track_names = []\n",
    "tracks = []\n",
    "for idx, track_filepath in enumerate(track_filepaths):\n",
    "    x, sr = torchaudio.load(track_filepath)\n",
    "    x = x[:, start_sample: end_sample]\n",
    "\n",
    "\n",
    "\n",
    "    for n in range(x.shape[0]):\n",
    "      x_sub = x[n:n+1, :]\n",
    "\n",
    "      gain_dB = np.random.rand() * 12\n",
    "      gain_dB *= np.random.choice([1.0, -1.0])\n",
    "      gain_ln = 10 ** (gain_dB/20.0)\n",
    "      x_sub *= gain_ln \n",
    "\n",
    "      tracks.append(x_sub)\n",
    "      track_names.append(os.path.basename(track_filepath))\n",
    "      IPython.display.display(ipd.Audio(x[n, :].view(1,-1).numpy(), rate=sr, normalize=True))    \n",
    "      print(idx+1, os.path.basename(track_filepath))\n",
    "\n",
    "# add dummy tracks of silence if needed\n",
    "if system.hparams.automix_model == \"mixwaveunet\" and len(tracks) < 8:\n",
    "    tracks.append(torch.zeros(x.shape))\n",
    "\n",
    "# stack tracks into a tensor\n",
    "tracks = torch.stack(tracks, dim=0)\n",
    "tracks = tracks.permute(1, 0, 2)\n",
    "# tracks have shape (1, num_tracks, seq_len)\n",
    "print(tracks.shape)\n",
    "\n",
    "# listen to the input (mono) before mixing\n",
    "input_mix = tracks.sum(dim=1, keepdim=True)\n",
    "input_mix /= input_mix.abs().max()\n",
    "print(input_mix.shape)\n",
    "plt.figure(figsize=(10, 2))\n",
    "librosa.display.waveshow(input_mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
    "plt.ylim([-1,1])\n",
    "plt.grid(c=\"lightgray\")\n",
    "plt.show()\n",
    "IPython.display.display(ipd.Audio(input_mix.view(1,-1).numpy(), rate=sr, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8apK_8kxdZ",
   "metadata": {
    "id": "5e8apK_8kxdZ"
   },
   "source": [
    "Above we can hear the tracks with a simple mono mix. Now we will create a mix with the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqRRxzlYWaFw",
   "metadata": {
    "id": "lqRRxzlYWaFw"
   },
   "outputs": [],
   "source": [
    "tracks = tracks.view(1,8,-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "  y_hat, p = system(tracks)\n",
    "  \n",
    "# view the mix\n",
    "print(y_hat.shape)\n",
    "y_hat /= y_hat.abs().max()\n",
    "plt.figure(figsize=(10, 2))\n",
    "librosa.display.waveshow(y_hat.view(2,-1).cpu().numpy(), sr=sr, zorder=3)\n",
    "plt.ylim([-1,1])\n",
    "plt.grid(c=\"lightgray\")\n",
    "plt.show()\n",
    "IPython.display.display(ipd.Audio(y_hat.view(2,-1).cpu().numpy(), rate=sr, normalize=True))\n",
    "\n",
    "# print the parameters\n",
    "if system.hparams.automix_model == \"dmc\":\n",
    "    for track_fp, param in zip(track_names, p.squeeze()):\n",
    "        print(os.path.basename(track_fp), param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2meIZ8Ck4UA",
   "metadata": {
    "id": "b2meIZ8Ck4UA"
   },
   "source": [
    "You should be able to hear that the levels have been adjusted and the sources panned to sound more like the original mix indicating that our system learned to overfit the songs in our very small training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b269307-6382-4541-af0a-768aa5945bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d167b1-9416-4ab6-b894-a868d79e43fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f550b43-bf8c-41fd-9f0e-729fdb546dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
