{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c8907d-91cf-4da0-93d5-15361d2038f8",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this notebook we will demonstrate how to evaluate a set of generated mixes via objective metrics.\n",
    "\n",
    "We will use the mixes generated from the [inference notebook](https://github.com/csteinmetz1/automix-toolkit/blob/main/notebooks/inference.ipynb), and we will objectively compare those mixes to the human-made groudn truth mixes.\n",
    "\n",
    "The objective evaluation of mixes can be carried out through audio features that relate to the most common audio effects used during mixing. Since audio effects generally manipulate audio characteristics such as frequency content, dynamics, spatialization, timbre, or pitch, we can use audio features that are associated with these audio characteristics as a way to numerically evaluate mixes.\n",
    "\n",
    "We can use the following audio features:\n",
    "    \n",
    "-**Spectral features** for EQ and reverberation: centroid, bandwidth, contrast, flatness, and roll-off\n",
    "\n",
    "-**Spatialisation features** for panning: the Panning Root Mean Square (RMS)\n",
    "\n",
    "-**Dynamic features** for dynamic range processors: RMS level, dynamic spread and crest factor\n",
    "\n",
    "-**Loudness features**: the integrated loudness level (LUFS) and peak loudness\n",
    "\n",
    "Note: This notebook assumes that you have already installed the `automix` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71210e5-3ef2-4f2f-a393-876e88aceac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automix.evaluation.utils_evaluation import get_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da649ee8-4418-41ec-82d8-851997388423",
   "metadata": {},
   "source": [
    "# Drums mixing evaluation\n",
    "\n",
    "We will evaluate two different trained models with a test sample from the ENST-drums subset.\n",
    "\n",
    "Models: the Differentiable Mixing Console (DMC), and the MixWaveUNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1155b-3143-4400-abbe-8212b27ed4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for the mixes to be evaluated \n",
    "\n",
    "mix_target_path = \"drums-test-rock/mix/dry_mix_066_phrase_rock_complex_fast_sticks.wav\"\n",
    "mix_auto_path_wun = \"drums-test-rock/mix/dry_mix_066_phrase_rock_complex_fast_sticks_MixWaveUNet.wav\"\n",
    "mix_auto_path_dmc = \"drums-test-rock/mix/dry_mix_066_phrase_rock_complex_fast_sticks_DMC.wav\"\n",
    "\n",
    "mix_target_path = os.path.join(os.path.realpath('..'), mix_target_path)\n",
    "mix_auto_path_wun = os.path.join(os.path.realpath('..'), mix_auto_path_wun)\n",
    "mix_auto_path_dmc = os.path.join(os.path.realpath('..'), mix_auto_path_dmc)\n",
    "\n",
    "\n",
    "# Global Settings\n",
    "SR = 44100\n",
    "max_samples = 30*SR\n",
    "start_sample = 0 * SR\n",
    "end_sample = start_sample + max_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab1e66-438a-4eff-a117-a77a9df37417",
   "metadata": {},
   "source": [
    "## Load the MixWaveUNet mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd57002-a40e-4a12-9860-b14b483b442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "target_audio, sr = torchaudio.load(mix_target_path)\n",
    "target_audio = target_audio[:, start_sample: end_sample]\n",
    "target_audio_to_plot = target_audio.view(1,-1).numpy()\n",
    "librosa.display.waveshow(target_audio_to_plot, x_axis='s', sr=SR, zorder=3, label='human-made', color='k')\n",
    "target_audio = target_audio.numpy()\n",
    "\n",
    "wun_audio, sr = torchaudio.load(mix_auto_path_wun)\n",
    "wun_audio = wun_audio[:, start_sample: end_sample]\n",
    "librosa.display.waveshow(wun_audio.view(1,-1).numpy(), x_axis='s', sr=SR, zorder=3, label='MixWaveUNet', color='c')\n",
    "wun_audio = wun_audio.numpy()\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2a04e-fdbc-4bd8-a0f3-f75f04c12b50",
   "metadata": {},
   "source": [
    "## Load the DMC mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96477615-b8e9-4eaa-a328-d099f8fad585",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "librosa.display.waveshow(target_audio_to_plot, x_axis='s', sr=SR, zorder=3, label='human-made', color='k')\n",
    "\n",
    "dmc_audio, sr = torchaudio.load(mix_auto_path_dmc)\n",
    "dmc_audio = dmc_audio[:, start_sample: end_sample]\n",
    "librosa.display.waveshow(dmc_audio.view(1,-1).numpy(), x_axis='s', sr=SR, zorder=3, label='DMC', color='m')\n",
    "dmc_audio = dmc_audio.numpy()\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2724bc-a9d3-4768-a5ce-053c00043e3d",
   "metadata": {},
   "source": [
    "## Compute the loudness, spectral, panning and dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d38a9-1f22-479d-b3f7-064f587ad9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wun_features = get_features(target_audio, wun_audio)\n",
    "dmc_features = get_features(target_audio, dmc_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56974ce3-c0dc-4661-aa40-de3625be1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, i in wun_features.items():\n",
    "    print(k, i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a691cf-5ba7-40af-847c-015ecc3928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, i in dmc_features.items():\n",
    "    print(k, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41291c06-22ec-49fe-8ccd-9662a350a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(*zip(*wun_features.items()), color='c', label='MixWaveUNet')\n",
    "plt.bar(*zip(*dmc_features.items()), color='m', label='DMC')\n",
    "plt.axvline(2.5, 0, 1, linestyle='--', alpha=0.5, color='k', linewidth=0.75)\n",
    "plt.axvline(10.5, 0, 1, linestyle='--', alpha=0.5, color='k', linewidth=0.75)\n",
    "plt.axvline(15.5, 0, 1, linestyle='--', alpha=0.5, color='k', linewidth=0.75)\n",
    "plt.xticks(rotation=-90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfa503-d2f2-4132-9e60-6bd8267ff8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3fa5f622e0c4f19de725eca262006d4f26f3d54faeda6e10ceb975b2274f74b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
