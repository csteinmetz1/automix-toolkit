{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa13a13",
   "metadata": {},
   "source": [
    "# Datasets for automix systems\n",
    "\n",
    "In this notebook, we will first discuss the datasets used to train the automix systems. Thereafter, we will see how to pre-process the data and set up the dataloaders for training the deep learning models for these systems.\n",
    "\n",
    "Training automix models requires paired multitrack stems and their corresponding mixdowns. Below listed are the desired properties for these datasets:\n",
    "\n",
    "1. __Time alligned stems and mixes__ : We require time-alligned stems and mixes to allow the models to learn timewise transformation relationships.\n",
    "\n",
    "2. __Diverse instrument categories__ : The more diverse the number of instruments in the dataset, the more likely is the trained system to perform well with real-world songs.\n",
    "\n",
    "3. __Diverse genres of songs__ : The mixing practices vary slightly from one genre to another. Hence, if the dataset has multitrack mixes from different genres, the trained system will be exposed to more diverse distribution of data.\n",
    "\n",
    "4. __Dry multitrack stems__ : Mixing involves processing the recorded dry stems for corrective and aesthetic reasons before summing them to form a cohesive mixture. For a model to learn the correct way to process the stems to generate mixes, we need to train it on dry unprocessed stems and mix pairs. However, more recently approaches to use processed stems from datasets like MUSEDB to train automix systems have been explored. These approaches use a pre-processing effect normalisation method to deal with pre-processed wet stems. For the scope of this tutorial, we do not discuss these methods. However, we recommend having a look at [this](https://arxiv.org/abs/2208.11428) paper being presented at ISMIR 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c3596",
   "metadata": {},
   "source": [
    "Here we list the datasets available for training automix systems. \n",
    "\n",
    "| Dataset | Size(Hrs) | no. of Songs | no. of Instrument Category | no. of tracks | Type | Usage Permissions | Other info | Remarks |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| [MedleyDB](https://medleydb.weebly.com/) | 7.2 | 122 | 82 | 1-26 | Multitrack, Wav | Open | 44.1KHz, 16 bit, stereo | - |\n",
    "| [ENST Drums](https://perso.telecom-paristech.fr/grichard/ENST-drums/) | 1.25 | - | 1 | 8 | Drums, Wav/AVI | Limited | 44.1KHz, 16 bit, stereo | Drums only dataset |\n",
    "| [Cambridge Multitrack](https://www.cambridge-mt.com/ms/mtk/) | >3 | >50 | >5 | 5-70 | Multitrack, Wav | open | 44.1KHz, 16/24 bit, Stereo | Not time alligned, recordings for all the songs are not uniform |\n",
    "| [MUSEDB](https://sigsep.github.io/datasets/musdb.html) | ~10 | 150 | 4 | 4 | Multitrack, Wav | open | 44.1KHz, Stereo | used mainly for source separation, wet stems |\n",
    "| [Slakh](http://www.slakh.com/) | 145 | 2100 | 34 | 4-48 | Synthesised, Flac,  | open | 44.1KHz, 16 bit, stereo | used mainly for source separation; sometimes wet stems |\n",
    "| [Shaking Through](https://weathervanemusic.org/shakingthrough/episode-index) | 4.5 | 68 | >30 | >40 | Multitrack, Wav | User only | 44.1/88.2KHz, 16/24 bit, stereo | - |\n",
    "| [BitMIDI](https://bitmidi.com/) | - | >1M | >5 | >5 | Multitrack MIDI | open | MIDI data | MIDI data submitted by users across world |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105690f8",
   "metadata": {},
   "source": [
    "For this tutorial, we will use ENST-drums for training Wave-U-Net and ENST-drums, DSD100, and MedleyDB for training Differentiable Mixing Console(DMC).\n",
    "\n",
    "In the following section, we will discuss the recommended pre-processing methods for these datasets and the methods to set up dataloaders for training the models. This notebook assumes that you have already installed the `automix` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad526e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d3ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from automix.data import dsd100, medleydb, drums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eab3d3",
   "metadata": {},
   "source": [
    "We define dataset classes for DSD100, MedleyDB, and ENSTdrums, and then use `getitem()` function to load the audio data into the dataloader for training and testing. \n",
    "\n",
    "### Listed below are few of the advised variables that you should define in the dataset class definition:\n",
    "\n",
    "1. __Root directory__ of the folder containing the dataset. \n",
    "2. __Length of the audio__ you wish to load for training/testing.\n",
    "3. __Sample rate__ at which you wish to load the audio data.\n",
    "\n",
    "### Pre-processing advice for loading multitrack data:\n",
    "\n",
    "1. Discard the examples from the dataset that have length shorter than the prescribed length.\n",
    "\n",
    "        ```\n",
    "        #code from automix/data/drums.py\n",
    "        #remove any mixes that are shorter than the requested length\n",
    "        self.mix_filepaths = [\n",
    "            fp\n",
    "            for fp in self.mix_filepaths\n",
    "            \n",
    "            # use torchaudio.info to get information about the audio. This is much faster than loading the whole audio.\n",
    "            if torchaudio.info(fp).num_frames > self.length\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "2. Loudness normalise the stems and the mixes after loading. \n",
    "\n",
    "        ```\n",
    "        #code from automix/data/drums.py\n",
    "        y /= y.abs().max().clamp(1e-8) \n",
    "        ```\n",
    "\n",
    "3. Look out for silence in the loaded audio: Common practice is to generate a random starting index for the frame from which the audio is loaded. However, it is likely that some of the multitrack stem or the mix as a whole could have just silence in this chunk of loaded audio. This results in generation of NaN in the audio tensor when it is normalised. In the below shown code block, we show how to check for silence. We keep generating a new starting index(`offset`)) for loading the audio until the audio has some content and is not just silence(`silent is False`).\n",
    "\n",
    "                ```\n",
    "                #code from automix/data/drums.py\n",
    "                # load the chunk of the mix\n",
    "                        silent = True\n",
    "                        while silent:\n",
    "                        # get random offset\n",
    "                        offset = np.random.randint(0, md.num_frames - self.length - 1)\n",
    "\n",
    "                        y, sr = torchaudio.load(\n",
    "                                mix_filepath,\n",
    "                                frame_offset=offset,\n",
    "                                num_frames=self.length,\n",
    "                        )\n",
    "                        energy = (y**2).mean()\n",
    "                        if energy > 1e-8:\n",
    "                                silent = False\n",
    "\n",
    "                        # only normalise the audio that are not silent\n",
    "                        y /= y.abs().max().clamp(1e-8)  # peak normalize\n",
    "                ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29c832",
   "metadata": {},
   "source": [
    "## ENST Drums\n",
    "\n",
    "Below described is the folder structure of the ENST Drums dataset:\n",
    "\n",
    "- ENST-Drums\n",
    "    - drummer_1\n",
    "        - annotation\n",
    "        - audio\n",
    "            - accompaniment\n",
    "            - dry mix\n",
    "            - hi-hat\n",
    "            - kick\n",
    "            - overhead L\n",
    "            - overhead R\n",
    "            - snare\n",
    "            - tom 1\n",
    "            - tom 2\n",
    "            - wet mix\n",
    "    - drummer_2\n",
    "        - (same structure as drummer_1)\n",
    "    - drummer_3\n",
    "        - (same structure as drummer_1)\n",
    "\n",
    "We are going to use audios from the wet mix folder for this tutorial. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSTDrumsDataset\n",
    "from automix.data.drums import ENSTDrumsDataset\n",
    "enst = ENSTDrumsDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf9585",
   "metadata": {},
   "source": [
    "In the automix/data/drums, we define an ENSTDrumsdataset class and use the `getitem()` to load data for the dataloader in our training loop.\n",
    "\n",
    "``` \n",
    "class ENSTDrumsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        length: int,\n",
    "        sample_rate: float,\n",
    "        drummers: List[int] = [1, 2],\n",
    "        track_names: List[str] = [\n",
    "            \"kick\",\n",
    "            \"snare\",\n",
    "            \"hi-hat\",\n",
    "            \"overhead_L\",\n",
    "            \"overhead_R\",\n",
    "            \"tom_1\",\n",
    "            \"tom_2\",\n",
    "            \"tom_3\",\n",
    "        ],\n",
    "        indices: Tuple[int, int] = [0, 1],\n",
    "        wet_mix: bool = False,\n",
    "        hits: bool = False,\n",
    "        num_examples_per_epoch: int = 1000,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "```\n",
    "- We use indices to define the train-test split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf01a0",
   "metadata": {},
   "source": [
    "- In the `getitem()` of the dataset class, we first generate a `mix_idx` which is a random number in the range of 0 and the number of songs in the directory(len of mix_filepaths). This allows to randomly pick a mix/song from the mix_filepath.\n",
    "    ```\n",
    "    def __getitem__(self, _):\n",
    "            # select a mix at random\n",
    "            mix_idx = np.random.randint(0, len(self.mix_filepaths))\n",
    "            mix_filepath = self.mix_filepaths[mix_idx]\n",
    "            example_id = os.path.basename(mix_filepath)\n",
    "            drummer_id = os.path.normpath(mix_filepath).split(os.path.sep)[-4]\n",
    "\n",
    "            md = torchaudio.info(mix_filepath)  # check length\n",
    "    ```\n",
    "- Next, we load the mix(`y`) from the filepath. Make sure to check for silence as discussed above. Once the mix is loaded, peak normalise it. \n",
    "    ```\n",
    "            # load the chunk of the mix\n",
    "            silent = True\n",
    "            while silent:\n",
    "                # get random offset\n",
    "                offset = np.random.randint(0, md.num_frames - self.length - 1)\n",
    "\n",
    "                y, sr = torchaudio.load(\n",
    "                    mix_filepath,\n",
    "                    frame_offset=offset,\n",
    "                    num_frames=self.length,\n",
    "                )\n",
    "                energy = (y**2).mean()\n",
    "                if energy > 1e-8:\n",
    "                    silent = False\n",
    "\n",
    "            y /= y.abs().max().clamp(1e-8)  # peak normalize\n",
    "    ```\n",
    "- Last step is to load the stems. `max_num_tracks` is the maximum number of tracks you want to load. Some songs might have less or more stems than this number. We keep a track of empty stems using `pad` which is `True` whenever the stem is empty. \n",
    "The `getitem()` returns stems tensor (`x`), mix (`y`), and `pad` information.\n",
    "\n",
    "        # -------------------- load the tracks from disk --------------------\n",
    "        x = torch.zeros((self.max_num_tracks, self.length))\n",
    "        pad = [True] * self.max_num_tracks  # note which tracks are empty\n",
    "\n",
    "        for tidx, track_name in enumerate(self.track_names):\n",
    "            track_filepath = os.path.join(\n",
    "                self.root_dir,\n",
    "                drummer_id,\n",
    "                \"audio\",\n",
    "                track_name,\n",
    "                example_id,\n",
    "            )\n",
    "            if os.path.isfile(track_filepath):\n",
    "                x_s, sr = torchaudio.load(\n",
    "                    track_filepath,\n",
    "                    frame_offset=offset,\n",
    "                    num_frames=self.length,\n",
    "                )\n",
    "                x_s /= x_s.abs().max().clamp(1e-6)\n",
    "                x_s *= 10 ** (-12 / 20.0)\n",
    "                x[tidx, :] = x_s\n",
    "                pad[tidx] = False\n",
    "\n",
    "        return x, y, pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdc647",
   "metadata": {},
   "source": [
    "## DSD100 dataset\n",
    "\n",
    "Below described is the folder structure of the DSD100 dataset: \n",
    "\n",
    "- ENST Drums\n",
    "    - Train\n",
    "        - Songdir(songname)\n",
    "            - vocals.wav\n",
    "            - bass.wav\n",
    "            - drums.wav\n",
    "            - other.wav\n",
    "            - accompaniment.wav\n",
    "            - mixture.wav\n",
    "    - Test\n",
    "         - Songdir(songname)\n",
    "            - vocals.wav\n",
    "            - bass.wav\n",
    "            - drums.wav\n",
    "            - other.wav\n",
    "            - accompaniment.wav\n",
    "            - mixture.wav\n",
    "            \n",
    "Note: Accompaniment is the sum of bass, drums, and other.\n",
    "\n",
    "For the purpose of training our models, we use:  \n",
    "\n",
    "__Input__: vocals, bass, drums, and other\n",
    "\n",
    "__Output__: Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5eee90",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will first define a dataset class and use the `getitem()` function to load items into the dataloader.\n",
    "\n",
    "```\n",
    "#Code from automix/data/dsd100.py\n",
    "\n",
    "class DSD100Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        length: int,\n",
    "        sample_rate: float,\n",
    "        indices: Tuple[int, int],\n",
    "        track_names: List[str] = [\"bass\", \"drums\", \"other\", \"vocals\"],\n",
    "        num_examples_per_epoch: int = 1000,\n",
    "    ) -> None:\n",
    "```\n",
    "\n",
    "\n",
    "Hereafter, we follow similar structure in `getitem()` as in the case of ENSTDrums.\n",
    "- We first pick a mix_filepath on random and then look for non-silent part to load the mix(`y`). \n",
    "- Then, we load stems(`x`) starting with the same start_idx of the prescribed length.  \n",
    "- We peak normalise all the loaded stems and mix and save the empty stem inofrmation in the `pad` variable.\n",
    "- We then return `x`, `y`, and `pad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSD100 dataset\n",
    "from automix.data.dsd100 import DSD100Dataset\n",
    "dsd100 = DSD100Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80e1f1",
   "metadata": {},
   "source": [
    "# MedleyDB Dataset\n",
    "\n",
    "Described below is the folder structure for MedleyDB:\n",
    "\n",
    "- MedleyDB\n",
    "    - songnames\n",
    "        - songname_MIX.wav\n",
    "        - songname_STEMS\n",
    "            - songname_STEMS_{stem_number}.wav\n",
    "        - songname_RAW\n",
    "            - songname_STEMS_{stem_number}_{track_number}.wav\n",
    "\n",
    "- STEMS folder have some of the RAW audio tracks combined into a single audio file.\n",
    "- RAW folder contains all of the audio tracks individually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3fa5f622e0c4f19de725eca262006d4f26f3d54faeda6e10ceb975b2274f74b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
