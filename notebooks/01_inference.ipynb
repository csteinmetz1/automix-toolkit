{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9998da2d",
      "metadata": {
        "id": "9998da2d"
      },
      "source": [
        "# Inference\n",
        "\n",
        "In this notebook we will demonstrate how to use two pretrained models to generate multitrack mixes of drum recordings. We provide models trained on the ENST-drums dataset, which features a few hundred drums multitracks and mixes of these multitracks made by professional audio engineers. We train two different multitrack mixing model architectures: the Differentiable Mixing Console (DMC), and the MixWaveUNet. First we will download the model checkpoints and some test audio, then load up the models and the audio tracks and generate a mix that we can listen to. \n",
        "\n",
        "Note: This notebook assumes that you have already installed the `automix` package. If you have not done so, you can run the following:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/csteinmetz1/automix-toolkit"
      ],
      "metadata": {
        "id": "v3f6mU0C_46L"
      },
      "id": "v3f6mU0C_46L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0aa016",
      "metadata": {
        "id": "bf0aa016"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "import IPython\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from automix.system import System"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4a372d",
      "metadata": {
        "id": "7b4a372d"
      },
      "source": [
        "## Download the pretrained models and multitracks\n",
        "First we will download two different pretrained models. Then we will also download a `.zip` file containing a drum multitrack and the demo mulitrack that were unseen during training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cafed4",
      "metadata": {
        "scrolled": true,
        "id": "a8cafed4"
      },
      "outputs": [],
      "source": [
        "# download the pretrained models for DMC and MixWaveUNet trained on ENST-drums dataset\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-dmc.ckpt\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-mixwaveunet.ckpt\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/medleydb-16-dmc.ckpt\n",
        "!mv enst-drums-dmc.ckpt checkpoints/enst-drums-dmc.ckpt\n",
        "!mv enst-drums-mixwaveunet.ckpt checkpoints/enst-drums-mixwaveunet.ckpt\n",
        "!mv medleydb-16-dmc.ckpt checkpoints/medleydb-16-dmc.ckpt\n",
        "\n",
        "# then download and extract a drum multitrack from the test set\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/drums-test-rock.zip\n",
        "!unzip -o drums-test-rock.zip\n",
        "\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/flare-dry-stems.zip\n",
        "!unzip -o flare-dry-stems.zip -d flare-dry-stems"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "KgGqAW3ZL9il"
      },
      "id": "KgGqAW3ZL9il",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9d072cd7",
      "metadata": {
        "id": "9d072cd7"
      },
      "source": [
        "## Set configuration\n",
        "We have the option to select one of two different checkpoints. \n",
        "\n",
        "If we select `enst-drums-dmc.ckpt` we can use the pretrained Differentiable mixing console model which will directly predict gain and panning parameters for each track. On the other hand we can also select `enst-drums-mixwaveunet.ckpt` which will use a multi-input WaveUNet to create a mix of the tracks. To make computation faster we can restrict the maximum number of samples the process with `max_samples`. Using the default `max_samples = 262144` will mix about the first 6 seconds of the track. You can try increasing this value to see how the results change. \n",
        "\n",
        "Note: In the case of MixWaveUNet, a power of 2 value for `max_samples` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94d33a8",
      "metadata": {
        "id": "e94d33a8"
      },
      "outputs": [],
      "source": [
        "track_dir = \"./drums-test-rock/tracks\"\n",
        "track_ext = \"wav\"\n",
        "\n",
        "dmc_ckpt_path = \"checkpoints/enst-drums-dmc.ckpt\"\n",
        "mwun_ckpt_path = \"checkpoints/enst-drums-mixwaveunet.ckpt\"\n",
        "\n",
        "max_samples = 262144"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140c2b9d",
      "metadata": {
        "id": "140c2b9d"
      },
      "source": [
        "## Load pretrained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c716659",
      "metadata": {
        "scrolled": false,
        "id": "7c716659"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "dmc_system = System.load_from_checkpoint(dmc_ckpt_path, pretrained_encoder=False, map_location=\"cpu\").eval()\n",
        "mwun_system = System.load_from_checkpoint(mwun_ckpt_path, map_location=\"cpu\").eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d91718",
      "metadata": {
        "id": "68d91718"
      },
      "source": [
        "## Load multitrack \n",
        "Now we will read the tracks from disk and create a tensor with all the tracks. In this case, we first peak normalize each track to -12 dB which is what the models expect. In the case of MixWaveUNet, we will add an extra track of silence if less than 8 are provided. However, the DMC model can accept any number of tracks, wether more or less than it was trained with.\n",
        "\n",
        "We can also create a simple mono mixture of these tracks to hear what the multitrack sounds like before we do any mixing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b8b386",
      "metadata": {
        "scrolled": false,
        "id": "80b8b386"
      },
      "outputs": [],
      "source": [
        "# load the input tracks\n",
        "track_filepaths = glob.glob(os.path.join(track_dir, f\"*.{track_ext}\"))\n",
        "track_filepaths = sorted(track_filepaths)\n",
        "tracks = []\n",
        "for idx, track_filepath in enumerate(track_filepaths):\n",
        "    x, sr = torchaudio.load(track_filepath)\n",
        "    x = x[:, : max_samples]\n",
        "    x /= x.abs().max().clamp(1e-8) # peak normalize\n",
        "    x *= 10 ** (-12/20.0) # set peak to -12 dB\n",
        "    tracks.append(x)\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    librosa.display.waveshow(x.view(-1).numpy(), sr=sr, zorder=3)\n",
        "    plt.title(f\"{idx+1} {os.path.basename(track_filepath)}\")\n",
        "    plt.ylim([-1,1])\n",
        "    plt.grid(c=\"lightgray\")\n",
        "    plt.show()\n",
        "    IPython.display.display(ipd.Audio(x.view(-1).numpy(), rate=sr, normalize=True))    \n",
        "\n",
        "# add dummy tracks of silence if needed\n",
        "if len(tracks) < 8:\n",
        "    tracks.append(torch.zeros(x.shape))\n",
        "\n",
        "# stack tracks into a tensor\n",
        "tracks = torch.stack(tracks, dim=0)\n",
        "tracks = tracks.permute(1, 0, 2)\n",
        "# tracks have shape (1, num_tracks, seq_len)\n",
        "\n",
        "# listen to the input (mono) before mixing\n",
        "input_mix = tracks.sum(dim=1, keepdim=True)\n",
        "print(input_mix.shape)\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Mono Mix\")\n",
        "librosa.display.waveshow(input_mix.view(-1).numpy(), sr=sr, zorder=3, color=\"tab:orange\")\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(input_mix.view(-1).numpy(), rate=sr, normalize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189db3a1",
      "metadata": {
        "id": "189db3a1"
      },
      "source": [
        "## Generate the DMC mix\n",
        "Now we can listen to the predicted mix. If we create a mix with the differentiable mixing console we can also print out the gain (in dB) and pan parameter for each track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6876223c",
      "metadata": {
        "id": "6876223c"
      },
      "outputs": [],
      "source": [
        "# pass tracks to the model and create a mix\n",
        "with torch.no_grad(): # no need to compute gradients\n",
        "    mix, params = dmc_system(tracks[:,:-1,:])\n",
        "print(mix.shape, params.shape)\n",
        "\n",
        "# view the mix\n",
        "mix /= mix.abs().max()\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Differentiable Mixing Console\")\n",
        "librosa.display.waveshow(mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(mix.view(2,-1).numpy(), rate=sr, normalize=True))\n",
        "\n",
        "for track_fp, param in zip(track_filepaths, params.squeeze()):\n",
        "    print(os.path.basename(track_fp), param)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac3fc58a",
      "metadata": {
        "id": "ac3fc58a"
      },
      "source": [
        "## Generate the Mix-Wave-U-Net Mix\n",
        "If we use the MixWaveUNet there are no parameters to show since this model uses a *direct transformation* method which does not use intermediate mixing parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f379cbfe",
      "metadata": {
        "id": "f379cbfe"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad(): # no need to compute gradients\n",
        "    mwun_mix, params = mwun_system(tracks)\n",
        "print(mix.shape, params.shape)\n",
        "\n",
        "# view the mix\n",
        "mwun_mix /= mwun_mix.abs().max()\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Mix-Wave-U-Net\")\n",
        "librosa.display.waveshow(mwun_mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(mwun_mix.view(2,-1).numpy(), rate=sr, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MedleyDB\n",
        "Now we will run DMC that was trained on MedleyDB, which includes many types of instruments. This model was trained with all songs that had 16 or less tracks."
      ],
      "metadata": {
        "id": "9qsX4O4lBOGl"
      },
      "id": "9qsX4O4lBOGl"
    },
    {
      "cell_type": "code",
      "source": [
        "dmc_ckpt_path = \"checkpoints/medleydb-16-dmc.ckpt\"\n",
        "\n",
        "# load pretrained model\n",
        "medley_dmc_system = System.load_from_checkpoint(dmc_ckpt_path, pretrained_encoder=False, map_location=\"cpu\").eval()\n"
      ],
      "metadata": {
        "id": "bgjF03VFBNmY"
      },
      "id": "bgjF03VFBNmY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tracks\n",
        "We will use the stems from the song that Gary mixed in the first part of the tutorial."
      ],
      "metadata": {
        "id": "9w0LVF12BhDt"
      },
      "id": "9w0LVF12BhDt"
    },
    {
      "cell_type": "code",
      "source": [
        "track_dir = \"./flare-dry-stems\"\n",
        "track_ext = \"wav\"\n",
        "\n",
        "start_sample = int(32 * 44100)\n",
        "end_sample = start_sample + int(40 * 44100)\n",
        "\n",
        "# load the input tracks\n",
        "track_filepaths = glob.glob(os.path.join(track_dir, f\"*.{track_ext}\"))\n",
        "track_filepaths = sorted(track_filepaths)\n",
        "tracks = []\n",
        "track_names = []\n",
        "for idx, track_filepath in enumerate(track_filepaths):\n",
        "    x, sr = torchaudio.load(track_filepath)\n",
        "\n",
        "    if \"Vocal\" in track_filepath or \"Bass\" in track_filepath:\n",
        "      x_L = x[0:1, start_sample:end_sample]\n",
        "      #x_L /= x_L.abs().max().clamp(1e-8) # peak normalize\n",
        "      #x_L *= 10 ** (-12/20.0) # set peak to -12 dB\n",
        "      tracks.append(x_L)\n",
        "      track_names.append(os.path.basename(track_filepath))\n",
        "\n",
        "    else:\n",
        "      x_L = x[0:1, start_sample:end_sample]\n",
        "      x_R = x[1:2, start_sample:end_sample]\n",
        "\n",
        "      #x_L /= x_L.abs().max().clamp(1e-8) # peak normalize\n",
        "      #x_L *= 10 ** (-12/20.0) # set peak to -12 dB\n",
        "\n",
        "      #x_R /= x_R.abs().max().clamp(1e-8) # peak normalize\n",
        "      #x_R *= 10 ** (-12/20.0) # set peak to -12 dB\n",
        "\n",
        "      tracks.append(x_L)\n",
        "      tracks.append(x_R)\n",
        "      track_names.append(os.path.basename(track_filepath) + \"-L\")\n",
        "      track_names.append(os.path.basename(track_filepath) + \"-R\")\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    librosa.display.waveshow(x_L.view(-1).numpy(), sr=sr, zorder=3)\n",
        "    plt.title(f\"{idx+1} {os.path.basename(track_filepath)}\")\n",
        "    plt.ylim([-1,1])\n",
        "    plt.grid(c=\"lightgray\")\n",
        "    plt.show()\n",
        "    IPython.display.display(ipd.Audio(x_L.view(-1).numpy(), rate=sr, normalize=True))    \n",
        "\n",
        "# stack tracks into a tensor\n",
        "tracks = torch.stack(tracks, dim=0)\n",
        "tracks = tracks.permute(1, 0, 2)\n",
        "# tracks have shape (1, num_tracks, seq_len)\n",
        "\n",
        "# listen to the input (mono) before mixing\n",
        "input_mix = tracks.sum(dim=1, keepdim=True).clamp(-1, 1)\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Mono Mix\")\n",
        "librosa.display.waveshow(input_mix.view(-1).numpy(), sr=sr, zorder=3, color=\"tab:orange\")\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(input_mix.view(-1).numpy(), rate=sr, normalize=False))"
      ],
      "metadata": {
        "id": "aiK0mXJVBeU1"
      },
      "id": "aiK0mXJVBeU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a gain and panning mix of these stems."
      ],
      "metadata": {
        "id": "FewHsb2SZDn1"
      },
      "id": "FewHsb2SZDn1"
    },
    {
      "cell_type": "code",
      "source": [
        "# pass tracks to the model and create a mix\n",
        "with torch.no_grad(): # no need to compute gradients\n",
        "    mix = medley_dmc_system.model.block_based_forward(tracks, 262144, 262144//2)\n",
        "#print(mix.shape, params.shape)\n",
        "\n",
        "# view the mix\n",
        "mix /= mix.abs().max()\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Differentiable Mixing Console\")\n",
        "librosa.display.waveshow(mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(mix.view(2,-1).numpy(), rate=sr, normalize=True))\n",
        "\n",
        "#for track_fp, param in zip(track_names, params.squeeze()):\n",
        "#    print(os.path.basename(track_fp), param)"
      ],
      "metadata": {
        "id": "pkJ13Pe-BqZy"
      },
      "id": "pkJ13Pe-BqZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly not a perfect mix, but notice that the model has learned to raise the level of the vocal, pan it to the center, and try to pan the other elements to the sides."
      ],
      "metadata": {
        "id": "TY5iUGhEZICN"
      },
      "id": "TY5iUGhEZICN"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DD8zBMYVZO9x"
      },
      "id": "DD8zBMYVZO9x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}