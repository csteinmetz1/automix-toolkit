{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9998da2d",
      "metadata": {
        "id": "9998da2d"
      },
      "source": [
        "# Inference\n",
        "\n",
        "In this notebook we will demonstrate how to use two pretrained models to generate multitrack mixes of drum recordings. We provide models trained on the ENST-drums dataset, which features a few hundred drums multitracks and mixes of these multitracks made by professional audio engineers. We train two different multitrack mixing model architectures: the Differentiable Mixing Console (DMC), and the MixWaveUNet. First we will download the model checkpoints and some test audio, then load up the models and the audio tracks and generate a mix that we can listen to. \n",
        "\n",
        "Note: This notebook assumes that you have already installed the `automix` package. If you have not done so, you can run the following:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/csteinmetz1/automix-toolkit"
      ],
      "metadata": {
        "id": "v3f6mU0C_46L"
      },
      "id": "v3f6mU0C_46L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0aa016",
      "metadata": {
        "id": "bf0aa016"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "import IPython\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from automix.system import System"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4a372d",
      "metadata": {
        "id": "7b4a372d"
      },
      "source": [
        "## Download the pretrained models and drum multitrack\n",
        "First we will download two different pretrained models. Then we will also download a `.zip` file containing a drum multitrack from the test dataset that unseen during training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8cafed4",
      "metadata": {
        "scrolled": true,
        "id": "a8cafed4"
      },
      "outputs": [],
      "source": [
        "# download the pretrained models for DMC and MixWaveUNet trained on ENST-drums dataset\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-dmc.ckpt\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-mixwaveunet.ckpt\n",
        "!mv enst-drums-dmc.ckpt checkpoints/enst-drums-dmc.ckpt\n",
        "!mv enst-drums-mixwaveunet.ckpt checkpoints/enst-drums-mixwaveunet.ckpt\n",
        "\n",
        "# then download and extract a drum multitrack from the test set\n",
        "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/drums-test-rock.zip\n",
        "!unzip -o drums-test-rock.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d072cd7",
      "metadata": {
        "id": "9d072cd7"
      },
      "source": [
        "## Set configuration\n",
        "We have the option to select one of two different checkpoints. \n",
        "\n",
        "If we select `enst-drums-dmc.ckpt` we can use the pretrained Differentiable mixing console model which will directly predict gain and panning parameters for each track. On the other hand we can also select `enst-drums-mixwaveunet.ckpt` which will use a multi-input WaveUNet to create a mix of the tracks. To make computation faster we can restrict the maximum number of samples the process with `max_samples`. Using the default `max_samples = 262144` will mix about the first 6 seconds of the track. You can try increasing this value to see how the results change. \n",
        "\n",
        "Note: In the case of MixWaveUNet, a power of 2 value for `max_samples` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94d33a8",
      "metadata": {
        "id": "e94d33a8"
      },
      "outputs": [],
      "source": [
        "track_dir = \"./drums-test-rock/tracks\"\n",
        "track_ext = \"wav\"\n",
        "\n",
        "dmc_ckpt_path = \"checkpoints/enst-drums-dmc.ckpt\"\n",
        "mwun_ckpt_path = \"checkpoints/enst-drums-mixwaveunet.ckpt\"\n",
        "\n",
        "max_samples = 262144"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140c2b9d",
      "metadata": {
        "id": "140c2b9d"
      },
      "source": [
        "## Load pretrained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c716659",
      "metadata": {
        "scrolled": false,
        "id": "7c716659"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "dmc_system = System.load_from_checkpoint(dmc_ckpt_path, pretrained_encoder=False, map_location=\"cpu\").eval()\n",
        "mwun_system = System.load_from_checkpoint(mwun_ckpt_path, map_location=\"cpu\").eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d91718",
      "metadata": {
        "id": "68d91718"
      },
      "source": [
        "## Load multitrack \n",
        "Now we will read the tracks from disk and create a tensor with all the tracks. In this case, we first peak normalize each track to -12 dB which is what the models expect. In the case of MixWaveUNet, we will add an extra track of silence if less than 8 are provided. However, the DMC model can accept any number of tracks, wether more or less than it was trained with.\n",
        "\n",
        "We can also create a simple mono mixture of these tracks to hear what the multitrack sounds like before we do any mixing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b8b386",
      "metadata": {
        "scrolled": false,
        "id": "80b8b386"
      },
      "outputs": [],
      "source": [
        "# load the input tracks\n",
        "track_filepaths = glob.glob(os.path.join(track_dir, f\"*.{track_ext}\"))\n",
        "track_filepaths = sorted(track_filepaths)\n",
        "tracks = []\n",
        "for idx, track_filepath in enumerate(track_filepaths):\n",
        "    x, sr = torchaudio.load(track_filepath)\n",
        "    x = x[:, : max_samples]\n",
        "    x /= x.abs().max().clamp(1e-8) # peak normalize\n",
        "    x *= 10 ** (-12/20.0) # set peak to -12 dB\n",
        "    tracks.append(x)\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    librosa.display.waveshow(x.view(-1).numpy(), sr=sr, zorder=3)\n",
        "    plt.title(f\"{idx+1} {os.path.basename(track_filepath)}\")\n",
        "    plt.ylim([-1,1])\n",
        "    plt.grid(c=\"lightgray\")\n",
        "    plt.show()\n",
        "    IPython.display.display(ipd.Audio(x.view(-1).numpy(), rate=sr, normalize=True))    \n",
        "\n",
        "# add dummy tracks of silence if needed\n",
        "if len(tracks) < 8:\n",
        "    tracks.append(torch.zeros(x.shape))\n",
        "\n",
        "# stack tracks into a tensor\n",
        "tracks = torch.stack(tracks, dim=0)\n",
        "tracks = tracks.permute(1, 0, 2)\n",
        "# tracks have shape (1, num_tracks, seq_len)\n",
        "\n",
        "# listen to the input (mono) before mixing\n",
        "input_mix = tracks.sum(dim=1, keepdim=True)\n",
        "print(input_mix.shape)\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Mono Mix\")\n",
        "librosa.display.waveshow(input_mix.view(-1).numpy(), sr=sr, zorder=3, color=\"tab:orange\")\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(input_mix.view(-1).numpy(), rate=sr, normalize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189db3a1",
      "metadata": {
        "id": "189db3a1"
      },
      "source": [
        "## Generate the DMC mix\n",
        "Now we can listen to the predicted mix. If we create a mix with the differentiable mixing console we can also print out the gain (in dB) and pan parameter for each track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6876223c",
      "metadata": {
        "id": "6876223c"
      },
      "outputs": [],
      "source": [
        "# pass tracks to the model and create a mix\n",
        "with torch.no_grad(): # no need to compute gradients\n",
        "    mix, params = dmc_system(tracks[:,:-1,:])\n",
        "print(mix.shape, params.shape)\n",
        "\n",
        "# view the mix\n",
        "mix /= mix.abs().max()\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Differentiable Mixing Console\")\n",
        "librosa.display.waveshow(mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(mix.view(2,-1).numpy(), rate=sr, normalize=True))\n",
        "\n",
        "for track_fp, param in zip(track_filepaths, params.squeeze()):\n",
        "    print(os.path.basename(track_fp), param)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac3fc58a",
      "metadata": {
        "id": "ac3fc58a"
      },
      "source": [
        "## Generate the Mix-Wave-U-Net Mix\n",
        "If we use the MixWaveUNet there are no parameters to show since this model uses a *direct transformation* method which does not use intermediate mixing parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f379cbfe",
      "metadata": {
        "id": "f379cbfe"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad(): # no need to compute gradients\n",
        "    mwun_mix, params = mwun_system(tracks)\n",
        "print(mix.shape, params.shape)\n",
        "\n",
        "# view the mix\n",
        "mwun_mix /= mwun_mix.abs().max()\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.title(\"Mix-Wave-U-Net\")\n",
        "librosa.display.waveshow(mwun_mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
        "plt.ylim([-1,1])\n",
        "plt.grid(c=\"lightgray\")\n",
        "plt.show()\n",
        "IPython.display.display(ipd.Audio(mwun_mix.view(2,-1).numpy(), rate=sr, normalize=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}