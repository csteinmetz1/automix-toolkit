{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9998da2d",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this notebook we will demonstrate how to use two pretrained models to generate multitrack mixes of drum recordings. We provide models trained on the ENST-drums dataset, which features a few hundred drums multitracks and mixes of these multitracks made by professional audio engineers. We train two different multitrack mixing model architectures: the Differentiable Mixing Console (DMC), and the MixWaveUNet. First we will download the model checkpoints and some test audio, then load up the models and the audio tracks and generate a mix that we can listen to. \n",
    "\n",
    "Note: This notebook assumes that you have already installed the `automix` package. If you have not done so, you can run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0aa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automix.system import System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a372d",
   "metadata": {},
   "source": [
    "## Download the pretrained models and drum multitrack\n",
    "First we will download two different pretrained models. Then we will also download a `.zip` file containing a drum multitrack from the test dataset that unseen during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cafed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the pretrained models for DMC and MixWaveUNet trained on ENST-drums dataset\n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-dmc.ckpt\n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/enst-drums-mixwaveunet.ckpt\n",
    "!mv enst-drums-dmc.ckpt ../checkpoints/enst-drums-dmc.ckpt\n",
    "!mv enst-drums-mixwaveunet.ckpt ../checkpoints/enst-drums-mixwaveunet.ckpt\n",
    "\n",
    "# then download and extract a drum multitrack from the test set\n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/drums-test-rock.zip\n",
    "!unzip drums-test-rock.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d072cd7",
   "metadata": {},
   "source": [
    "## Set configuration\n",
    "We have the option to select one of two different checkpoints. \n",
    "\n",
    "If we select `enst-drums-dmc.ckpt` we can use the pretrained Differentiable mixing console model which will directly predict gain and panning parameters for each track. On the other hand we can also select `enst-drums-mixwaveunet.ckpt` which will use a multi-input WaveUNet to create a mix of the tracks. To make computation faster we can restrict the maximum number of samples the process with `max_samples`. Using the default `max_samples = 262144` will mix about the first 6 seconds of the track. You can try increasing this value to see how the results change. \n",
    "\n",
    "Note: In the case of MixWaveUNet, a power of 2 value for `max_samples` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dir = \"./drums-test-rock/tracks\"\n",
    "track_ext = \"wav\"\n",
    "\n",
    "#ckpt_path = \"../checkpoints/enst-drums-dmc.ckpt\"\n",
    "ckpt_path = \"../checkpoints/enst-drums-mixwaveunet.ckpt\"\n",
    "\n",
    "max_samples = 262144"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c2b9d",
   "metadata": {},
   "source": [
    "## Load pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c716659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "system = System.load_from_checkpoint(ckpt_path, pretrained_encoder=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d91718",
   "metadata": {},
   "source": [
    "## Load multitrack \n",
    "Now we will read the tracks from disk and create a tensor with all the tracks. In this case, we first peak normalize each track to -12 dB which is what the models expect. In the case of MixWaveUNet, we will add an extra track of silence if less than 8 are provided. However, the DMC model can accept any number of tracks, wether more or less than it was trained with.\n",
    "\n",
    "We can also create a simple mono mixture of these tracks to hear what the multitrack sounds like before we do any mixing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8b386",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the input tracks\n",
    "track_filepaths = glob.glob(os.path.join(track_dir, f\"*.{track_ext}\"))\n",
    "track_filepaths = sorted(track_filepaths)\n",
    "tracks = []\n",
    "for idx, track_filepath in enumerate(track_filepaths):\n",
    "    x, sr = torchaudio.load(track_filepath)\n",
    "    x = x[:, : max_samples]\n",
    "    x /= x.abs().max().clamp(1e-8) # peak normalize\n",
    "    x *= 10 ** (-12/20.0) # set peak to -12 dB\n",
    "    tracks.append(x)\n",
    "\n",
    "    IPython.display.display(ipd.Audio(x.view(1,-1).numpy(), rate=sr, normalize=True))    \n",
    "    print(idx+1, os.path.basename(track_filepath))\n",
    "\n",
    "# add dummy tracks of silence if needed\n",
    "if system.hparams.automix_model == \"mixwaveunet\" and len(tracks) < 8:\n",
    "    tracks.append(torch.zeros(x.shape))\n",
    "\n",
    "# stack tracks into a tensor\n",
    "tracks = torch.stack(tracks, dim=0)\n",
    "tracks = tracks.permute(1, 0, 2)\n",
    "# tracks have shape (1, num_tracks, seq_len)\n",
    "\n",
    "# listen to the input (mono) before mixing\n",
    "input_mix = tracks.sum(dim=1, keepdim=True)\n",
    "print(input_mix.shape)\n",
    "plt.figure(figsize=(10, 2))\n",
    "librosa.display.waveshow(input_mix.view(1,-1).numpy(), sr=sr, zorder=3)\n",
    "plt.ylim([-1,1])\n",
    "plt.grid(c=\"lightgray\")\n",
    "plt.show()\n",
    "IPython.display.display(ipd.Audio(input_mix.view(1,-1).numpy(), rate=sr, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189db3a1",
   "metadata": {},
   "source": [
    "## Generate the mix\n",
    "Now we can listen to the predicted mix. If we create a mix with the differentiable mixing console we can also print out the gain (in dB) and pan parameter for each track. If we use the MixWaveUNet there are no parameters to show since this model uses a *direct transformation* method which does not use intermediate mixing parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass tracks to the model and create a mix\n",
    "with torch.no_grad(): # no need to compute gradients\n",
    "    mix, params = system(tracks)\n",
    "print(mix.shape, params.shape)\n",
    "\n",
    "# view the mix\n",
    "mix /= mix.abs().max()\n",
    "plt.figure(figsize=(10, 2))\n",
    "librosa.display.waveshow(mix.view(2,-1).numpy(), sr=sr, zorder=3)\n",
    "plt.ylim([-1,1])\n",
    "plt.grid(c=\"lightgray\")\n",
    "plt.show()\n",
    "IPython.display.display(ipd.Audio(mix.view(2,-1).numpy(), rate=sr, normalize=True))\n",
    "\n",
    "if system.hparams.automix_model == \"dmc\":\n",
    "    for track_fp, param in zip(track_filepaths, params.squeeze()):\n",
    "        print(os.path.basename(track_fp), param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
