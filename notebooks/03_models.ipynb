{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0f574c",
   "metadata": {
    "id": "ac0f574c"
   },
   "source": [
    "# Models\n",
    "In this notebook we will dig into how the two automatic mixing models we discussed can be implemented in PyTorch.\n",
    "As usual, we will assume you have already installed the `automix` package from automix-toolkit. \n",
    "If not you can do it with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BPszHUPAD4x",
   "metadata": {
    "id": "1BPszHUPAD4x"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/csteinmetz1/automix-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1427312",
   "metadata": {
    "id": "d1427312"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from automix.utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070c1ac",
   "metadata": {
    "id": "e070c1ac"
   },
   "source": [
    "# MixWaveUNet\n",
    "First, we will take a look at the [Mix-Wave-U-Net](https://www.aes.org/e-lib/browse.cfm?elib=21023). Recall that this model is based on [Wave-U-Net](https://arxiv.org/abs/1806.03185) a time domain audio source separation model that is itself based on the famous [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) architecture. \n",
    "\n",
    "The overall architecture for the network is comprised of two types of blocks: the Downsampling blocks (shown on the left) and the Upsampling blocks (shown on the right). In the network we apply a certain number of these blocks, downsampling and then upsampling the signal at different temporal resolutions. Unique to U-Net like architectuers is the characteratistic skip connections that carry information from the each level in the downsampling branch to the respective branch in the upsampling brach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73bc24",
   "metadata": {
    "id": "ec73bc24"
   },
   "source": [
    "<img width = \"70%\" src=\"https://csteinmetz1.github.io/automix-toolkit/docs/assets/mix-wave-u-net.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1be93a",
   "metadata": {
    "id": "6e1be93a"
   },
   "source": [
    "We can start by importing the `MixWaveUNet` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e8899",
   "metadata": {
    "id": "917e8899"
   },
   "outputs": [],
   "source": [
    "from automix.models.mixwaveunet import MixWaveUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b0356",
   "metadata": {
    "id": "433b0356"
   },
   "source": [
    "Then we can construct this model supplying the desired hyperparameters.\n",
    "Below we will create a version of the model that accepts 8 input channels and produces a stereo (2) mix. We will use the default downsampling and upsampling kernel size of 13 and use a kernel size of 5 for the final output convolution. As in the original MixWaveUNet we use 12 down and upsampling blocks and increase the number of convolutional channels by 24 at each block. We also have the option to use either additive \"add\" or concatative \"concat\" skip connections. In this case, we will follow the original model and use concatenation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bc361",
   "metadata": {
    "id": "8b9bc361"
   },
   "outputs": [],
   "source": [
    "model = MixWaveUNet( \n",
    "    ninputs = 8,    # the number of input recordings we can mix (this is fixed at training)\n",
    "    noutputs = 2,   # the number of channels for the mix. Normally this is 2 for stereo mix\n",
    "    ds_kernel = 13, # kernel size for the convolutional layers in the Downsampling Blocks\n",
    "    us_kernel = 13, # kernel size for the convolutional layers in the Upsampling Blocks\n",
    "    out_kernel = 5, # kernel size for the convolutional layer in the final layer\n",
    "    layers = 12,    # Number of blocks in the upsampling and downsampling paths\n",
    "    ch_growth = 24, # Number convolutional channels to add at each layer\n",
    "    skip = \"concat\" # We can use either \"add\" or \"concat\" skip connections. (\"add\" will save parameters and memory)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2160da",
   "metadata": {
    "id": "1a2160da"
   },
   "source": [
    "Then we can count the number of parameters in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809cb15",
   "metadata": {
    "id": "f809cb15"
   },
   "outputs": [],
   "source": [
    "print(f\"{count_parameters(model)/1e6:0.3f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2d3ea",
   "metadata": {
    "id": "07e2d3ea"
   },
   "source": [
    "This model is currently untrained, but we can demonstrate how we can generate a mix from this model. The model expects as input a tensor of shape (batch_size, num_tracks, seq_len). In this case `num_tracks` is equal to the number of input recordings that we want to mix together and `seq_len` corresponds to the number of samples in each recording. Since we will stack this into a single tensor it requires that each recording is of the same length. Let's consider the following example of how we could generate mix from this untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6815d",
   "metadata": {
    "id": "cae6815d"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_tracks = 8\n",
    "seq_length = 262144\n",
    "\n",
    "x = torch.randn(batch_size, num_tracks, seq_length)\n",
    "y_hat, p = model(x)\n",
    "\n",
    "print(x.shape, y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba106f47",
   "metadata": {
    "id": "ba106f47"
   },
   "source": [
    "You can see that after passing in 8 tracks we will get two stereo mixes, one for each of the batch items. Importantly, note that calling `model(x)` will return two values. The first, `y_hat` is the mixture. The second value representas the parameters that created the mix. However, the parameters will only be populated for models that use explicit parameters like the DMC. So in this case we can see that `p` is a zero tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec92528",
   "metadata": {
    "id": "7ec92528"
   },
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd873",
   "metadata": {
    "id": "3a9cd873"
   },
   "source": [
    "Now that we understand the basic operation of the Mix-Wave-U-Net at a high level, let's investigate the inner workings of the model. To do so, we will define and connect the inner components of the original `MixWaveUNet` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93616846",
   "metadata": {
    "id": "93616846"
   },
   "source": [
    "## Downsampling Block\n",
    "Let's start with the `DownsamplingBlock`. Here we have reproduced the implementation from the `automix` package. \n",
    "It is composed of a few basic submodules. It starts with a `Conv1d`, `BatchNorm1d`, then `PReLU` activation, and a final `Conv1d` that has `stride=2`. While the original Wave-U-Net used decimation to downsample we can also use a strided convolution to achieve a similar downsampling operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe488f6",
   "metadata": {
    "id": "6fe488f6"
   },
   "outputs": [],
   "source": [
    "class DownsamplingBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ch_in: int,\n",
    "        ch_out: int,\n",
    "        kernel_size: int = 15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert kernel_size % 2 != 0  # kernel must be odd length\n",
    "        padding = kernel_size // 2  # calculate same padding\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.bn = torch.nn.BatchNorm1d(ch_out)\n",
    "        self.prelu = torch.nn.PReLU(ch_out)\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            ch_out,\n",
    "            ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        x_ds = self.conv2(x)\n",
    "        return x_ds, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09147fda",
   "metadata": {
    "id": "09147fda"
   },
   "source": [
    "Note that the `forward()` will return two tensors:\n",
    "\n",
    "- `x_ds` - the downsampled (by factor of 2) signal.\n",
    "- `x` - the processed signal before any downsampling. \n",
    "\n",
    "This is so we can save the itermediate tensors before downsampling so they can be used in the upsampling process when we employ the skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158e34e",
   "metadata": {
    "id": "f158e34e"
   },
   "source": [
    "## Upsampling Block\n",
    "As we see below, the `UpsamplingBlock` follows a very similar pattern to the `DownsamplingBlock` with the exceptation that is upsamples the signals (of course) but also that we have to handle aggregating information from the skip connections. As before we have a similar series connection of convolution, batch normalization, activations, and in this case a linear upsampling block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14461d6",
   "metadata": {
    "id": "b14461d6"
   },
   "outputs": [],
   "source": [
    "class UpsamplingBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ch_in: int,\n",
    "        ch_out: int,\n",
    "        kernel_size: int = 5,\n",
    "        skip: str = \"add\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert kernel_size % 2 != 0  # kernel must be odd length\n",
    "        padding = kernel_size // 2  # calculate same padding\n",
    "\n",
    "        self.skip = skip\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.bn = torch.nn.BatchNorm1d(ch_out)\n",
    "        self.prelu = torch.nn.PReLU(ch_out)\n",
    "        self.us = torch.nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor):\n",
    "        x = self.us(x)  # upsample by x2\n",
    "\n",
    "        # handle skip connections\n",
    "        if self.skip == \"add\":\n",
    "            x = x + skip\n",
    "        elif self.skip == \"concat\":\n",
    "            x = torch.cat((x, skip), dim=1)\n",
    "        elif self.skip == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c95b91",
   "metadata": {
    "id": "50c95b91"
   },
   "source": [
    "We can see here a unique part of the `forward()` in the `UpsamplingBlock` is that takes as input two tensors:\n",
    "\n",
    "- `x` - the output of the previous upsampling layer. \n",
    "- `skip` - the output of the respective downsampling layer (same resolution) which creates a skip connection.\n",
    "\n",
    "We can then see that we first upsample the input tensor `x` and then combine it with the `skip` connection. In our implementation we include a few different options. In the case of `\"add\"` we will simply sum the two tensors which is a pointwise sum between the signals in each channel. This is not as expressive but saves memory and lower the parameter count. In the `\"concat\"` case we will concatenate the two tensors along the channel dimension which will result in a new tensor that has twice the number of channels. However, this provides more flexibility since the convolutional layer that follows can decide how to mix these signals together. Finally, there is also the option to forgo the skip connections.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891368",
   "metadata": {
    "id": "a6891368"
   },
   "source": [
    "## Encoder\n",
    "Now let's use these building blocks to construct the Mix-Wave-U-Net.  We will start with the Encoder, which is composed of series connection of `DownsamplingBlocks`. We will use a `for` loop to construct each layer and then store them in a `ModuleList`. At the first layer, we will ensure the convolution accepts that same number of channels as there are input recordings (`ninputs`). For the other blocks, we will increase the number of channels by `ch_growth` each iteration (`ch_growth = 24`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14df6c7",
   "metadata": {
    "id": "c14df6c7"
   },
   "outputs": [],
   "source": [
    "ninputs = 8     # the number of input recordings we can mix (this is fixed at training)\n",
    "noutputs = 2    # the number of channels for the mix. Normally this is 2 for stereo mix\n",
    "ds_kernel = 13  # kernel size for the convolutional layers in the Downsampling Blocks\n",
    "us_kernel = 13  # kernel size for the convolutional layers in the Upsampling Blocks\n",
    "out_kernel = 5  # kernel size for the convolutional layer in the final layer\n",
    "layers = 12     # Number of blocks in the upsampling and downsampling paths\n",
    "ch_growth = 24  # Number convolutional channels to add at each layer\n",
    "skip = \"concat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be3bc4",
   "metadata": {
    "id": "68be3bc4"
   },
   "outputs": [],
   "source": [
    "encoder = torch.nn.ModuleList()\n",
    "\n",
    "for n in np.arange(layers):\n",
    "    if n == 0:\n",
    "        ch_in = ninputs\n",
    "        ch_out = ch_growth\n",
    "    else:\n",
    "        ch_in = ch_out\n",
    "        ch_out = ch_in + ch_growth\n",
    "\n",
    "    encoder.append(DownsamplingBlock(ch_in, ch_out, kernel_size=ds_kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e207bd",
   "metadata": {
    "id": "65e207bd"
   },
   "source": [
    "And now we can see the layers we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f16f6",
   "metadata": {
    "id": "083f16f6"
   },
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb349dd",
   "metadata": {
    "id": "4eb349dd"
   },
   "source": [
    "## Embedding/Latent\n",
    "In the middle of the network we will include a single convolutional layer to produce the latent embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfc5d9",
   "metadata": {
    "id": "7cdfc5d9"
   },
   "outputs": [],
   "source": [
    "embedding = torch.nn.Conv1d(ch_out, ch_out, kernel_size=1)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c2b52",
   "metadata": {
    "id": "e82c2b52"
   },
   "source": [
    "## Decoder\n",
    "In a similar manner to the encoder, we will construct the decoder by storing `UpsamplingBlocks` in a `ModuleList`. However, in this case we will count backwards (`step=-1`) as we create the layers, starting with the number of channels in the final layer of the encoder, decrementing this value by `ch_growth` at each iteration. Note also that when we use `\"concat\"` skip connections we will double the number of channels in each block to accomidate the additional channels from the skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30520a9",
   "metadata": {
    "id": "e30520a9"
   },
   "outputs": [],
   "source": [
    "decoder = torch.nn.ModuleList()\n",
    "for n in np.arange(layers, stop=0, step=-1):\n",
    "\n",
    "    ch_in = ch_out\n",
    "    ch_out = ch_in - ch_growth\n",
    "\n",
    "    if ch_out < ch_growth:\n",
    "        ch_out = ch_growth\n",
    "\n",
    "    if skip == \"concat\":\n",
    "        ch_in *= 2\n",
    "\n",
    "    decoder.append(\n",
    "        UpsamplingBlock(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size=us_kernel,\n",
    "            skip=skip,\n",
    "        )\n",
    "    )\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb3a68",
   "metadata": {
    "id": "80eb3a68"
   },
   "source": [
    "## Output\n",
    "Finally we have the output convolution which will collect the output channels from the final layer of the decoder and map them to the stereo mixture (`noutputs=2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623181c",
   "metadata": {
    "id": "9623181c"
   },
   "outputs": [],
   "source": [
    "output_conv = torch.nn.Conv1d(\n",
    "    ch_out + ninputs,\n",
    "    noutputs,\n",
    "    kernel_size=out_kernel,\n",
    "    padding=out_kernel // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0c06a",
   "metadata": {
    "id": "0ee0c06a"
   },
   "source": [
    "## Forward\n",
    "The forward pass of the modell involves simply iterating over the blocks in the encoder and storing the outputs as well as the skip connections. We will just use a list to store these tensors. Then we pass the final output from the encoder to the `embedding()` layer and use another `for` loop to iterate over the blocks in the decoder. This time we process the signals each time passing the respective skip connection. We use `skips.pop()` to return the last skip connection from the encoder (LIFO). Finally we implement the last skip connection which uses the original input recordings `x_in`. Again, recall that we return a `torch.zeros(1)` as dummy tensor since this model does not give us interpretable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b057362",
   "metadata": {
    "id": "0b057362"
   },
   "outputs": [],
   "source": [
    "def forward(x: torch.Tensor): \n",
    "    x_in = x\n",
    "    skips = [] # storage \n",
    "\n",
    "    for enc in encoder:\n",
    "        x, skip = enc(x)\n",
    "        skips.append(skip)\n",
    "\n",
    "    x = embedding(x)\n",
    "\n",
    "    for dec in decoder:\n",
    "        skip = skips.pop()\n",
    "        x = dec(x, skip)\n",
    "\n",
    "    x = torch.cat((x_in, x), dim=1)\n",
    "    y = output_conv(x)\n",
    "\n",
    "    return y, torch.zeros(1)  # return dummy parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad45fb",
   "metadata": {
    "id": "a9ad45fb"
   },
   "source": [
    "We can then test the model just as we did before and see the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2ca84",
   "metadata": {
    "id": "4bd2ca84"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_tracks = 8\n",
    "seq_length = 262144\n",
    "\n",
    "x = torch.randn(batch_size, num_tracks, seq_length)\n",
    "y_hat, p = forward(x)\n",
    "\n",
    "print(x.shape, y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435a854",
   "metadata": {
    "id": "4435a854"
   },
   "source": [
    "# Differentiable Mixing Console (DMC)\n",
    "Now that we have seen how the Mix-Wave-U-Net, a direct transformation approach, can be implemented, we will shift our focus to the [Differentiable Mixing Console](), which is a parameter estimation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28620160",
   "metadata": {
    "id": "28620160"
   },
   "source": [
    "<img width = \"100%\" src=\"https://csteinmetz1.github.io/automix-toolkit/docs/assets/dmc.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4349b7",
   "metadata": {
    "id": "5f4349b7"
   },
   "source": [
    "Similar to our explanation before we will first inspect the model from a high level and then go through the basic compotnents of the model so we can get an understanding of their operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933c280",
   "metadata": {
    "id": "8933c280"
   },
   "outputs": [],
   "source": [
    "from automix.models.dmc import DifferentiableMixingConsole, PostProcessor, Mixer, ShortChunkCNN_Res\n",
    "from automix.utils import restore_from_0to1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b171b",
   "metadata": {
    "id": "b60b171b"
   },
   "source": [
    "First we will create the main modules of the system.\n",
    "\n",
    "- `ShortChunkCNN_res` - This is our encoder. We use an encoder that operates on melspectrograms and has been pretrained. \n",
    "\n",
    "- `PostProcessor` - This is a MLP that will project our embeddings to the parameters of the mixing console.\n",
    "\n",
    "- `Mixer` - The differnetiable mixer class. In this case our mixer supports gain and stereo panning operations.\n",
    "\n",
    "We will also need to download the pretrained model checkpoint for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025d101",
   "metadata": {
    "id": "6025d101"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "# download the pretrained models for the encoder\n",
    "!wget https://huggingface.co/csteinmetz1/automix-toolkit/resolve/main/encoder.ckpt\n",
    "!mv encoder.ckpt checkpoints/encoder.ckpt\n",
    "encoder_ckpt_path = \"checkpoints/encoder.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e931d8f",
   "metadata": {
    "id": "9e931d8f"
   },
   "source": [
    "## Encoder\n",
    "The role of the encoder is extract information from each input recording that will be used in order to create a mix. This implicitly involves determining the identity of the source (e.g. drums, guitar, vocal, etc.) as well as other factors such as the level. We adopt a very standard 2d convolutional network that operates on log melspectrograms. In the original paper that authors used the [VGGish](https://github.com/harritaylor/torchvggish) architecture pretrained on [AudioSet](https://research.google.com/audioset/). \n",
    "\n",
    "To faciliate faster training and simpler code we opt to the [Short Chunk CNN](https://github.com/minzwon/sota-music-tagging-models) (with residual connections) which is very similar but faciliates easy computation of melspectrograms with [torchaudio](https://pytorch.org/audio/stable/index.html). In addition, we use a pretrained checkpoint after training the model on a music tagging task, which should aid in learning. We will not go into detail of how the encoder itself is implemented but you can see the details [here](automix/models/dmc.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c184634",
   "metadata": {
    "id": "1c184634"
   },
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "encoder = ShortChunkCNN_Res(sample_rate, ckpt_path=encoder_ckpt_path)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6397c0",
   "metadata": {
    "id": "ba6397c0"
   },
   "source": [
    "## Post-Processor\n",
    "The role of the Post-Processor is to take the track embedding and context embedding (for each track and context pair) and compute a set of control parameters for the current track. We can implement this as a simple multi-layer perceptron (MLP) with three layers. This network will use a sigmoid activation function to map all outputs between 0 and 1. This is the format our `Mixer` expects. Inside the `Mixer` these parameters will be denormalized to the correct ranges. In the original paper the authors use a tanh activation so that parameters are scaled between -1 and 1, but this is just a design choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a41cb",
   "metadata": {
    "id": "6a1a41cb"
   },
   "outputs": [],
   "source": [
    "class PostProcessor(torch.nn.Module):\n",
    "    def __init__(self, num_params: int, d_embed: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_embed, 256),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(256, 256),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Linear(256, num_params),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        return self.mlp(z)\n",
    "    \n",
    "postprocessor = PostProcessor(2, 2 * encoder.d_embed)\n",
    "print(postprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65170a9c",
   "metadata": {
    "id": "65170a9c"
   },
   "source": [
    "## Mixer\n",
    "The role of the mixer is to process the individual channels in a mix given the control parameters and produce a stereo mix. In the original paper the mixer included the Transformation Network. This network was first pretrained to emulate common audio effects like an equalizer, compressor, and reveberation model. In our setup we will consider the simple case for the Transformation Network which uses only gain (level) and panning parameters. Since these operations are differentiable we do not need to worry about the proxy method or any other differentiable signal processing techniques. \n",
    "\n",
    "As you can see below we will implement the differentiable mixer by simply applying the gain and panning operations, which enables the use of autodiff for the gradient computation during training. This is both fast and memory efficient. An extension of our implemntation could add in more effects like equalization or compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b9b68",
   "metadata": {
    "id": "f15b9b68"
   },
   "outputs": [],
   "source": [
    "class Mixer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: float,\n",
    "        min_gain_dB: int = -48.0,\n",
    "        max_gain_dB: int = 24.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_params = 2\n",
    "        self.param_names = [\"Gain dB\", \"Pan\"]\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_gain_dB = min_gain_dB\n",
    "        self.max_gain_dB = max_gain_dB\n",
    "\n",
    "    def forward(self, x: torch.Tensor, p: torch.Tensor):\n",
    "        \"\"\"Generate a mix of stems given mixing parameters normalized to (0,1).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Batch of waveform stem tensors with shape (bs, num_tracks, seq_len).\n",
    "            p (torch.Tensor): Batch of normalized mixing parameters (0,1) for each stem with shape (bs, num_tracks, num_params)\n",
    "\n",
    "        Returns:\n",
    "            y (torch.Tensor): Batch of stereo waveform mixes with shape (bs, 2, seq_len)\n",
    "        \"\"\"\n",
    "        bs, num_tracks, seq_len = x.size()\n",
    "\n",
    "        # ------------- apply gain -------------\n",
    "        gain_dB = p[..., 0]  # get gain parameter\n",
    "        gain_dB = restore_from_0to1(gain_dB, self.min_gain_dB, self.max_gain_dB)\n",
    "        gain_lin = 10 ** (gain_dB / 20.0)  # convert gain from dB scale to linear\n",
    "        gain_lin = gain_lin.view(bs, num_tracks, 1)  # reshape for multiplication\n",
    "        x = x * gain_lin  # apply gain (bs, num_tracks, seq_len)\n",
    "\n",
    "        # ------------- apply panning -------------\n",
    "        # expand mono stems to stereo, then apply panning\n",
    "        x = x.view(bs, num_tracks, 1, -1)  # (bs, num_tracks, 1, seq_len)\n",
    "        x = x.repeat(1, 1, 2, 1)  # (bs, num_tracks, 2, seq_len)\n",
    "\n",
    "        pan = p[..., 1]  # get pan parameter\n",
    "        pan_theta = pan * torch.pi / 2\n",
    "        left_gain = torch.cos(pan_theta)\n",
    "        right_gain = torch.sin(pan_theta)\n",
    "        pan_gains_lin = torch.stack([left_gain, right_gain], dim=-1)\n",
    "        pan_gains_lin = pan_gains_lin.view(bs, num_tracks, 2, 1)  # reshape for multiply\n",
    "        x = x * pan_gains_lin  # (bs, num_tracks, 2, seq_len)\n",
    "\n",
    "        # ----------------- apply mix -------------\n",
    "        # generate a mix for each batch item by summing stereo tracks\n",
    "        y = torch.sum(x, dim=1)  # (bs, 2, seq_len)\n",
    "\n",
    "        p = torch.cat(\n",
    "            (\n",
    "                gain_dB.view(bs, num_tracks, 1),\n",
    "                pan.view(bs, num_tracks, 1),\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        return y, p\n",
    "mixer = Mixer(sample_rate)\n",
    "print(mixer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e71a9d",
   "metadata": {
    "id": "43e71a9d"
   },
   "source": [
    "Here we will set up some inputs that we can use for our example. You can adjust these values to see how the results change. In this case we will use a batch size of 2, mixes with 8 input recordings each approx 3 sec in length (at a sample rate of 44100). Then we will generate a tensor of noise to represent theses tracks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ca586",
   "metadata": {
    "id": "1c7ca586"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_tracks = 4\n",
    "num_samples = 131072\n",
    "\n",
    "x = torch.randn(batch_size, num_tracks, num_samples)\n",
    "bs, num_tracks, seq_len = x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbb620",
   "metadata": {
    "id": "0bfbb620"
   },
   "source": [
    "### Generating embeddings\n",
    "As first the step we will need to generate embeddings with our encoder for each of the input recordings in each batch item.\n",
    "Since each batch item will contain multiple tracks (in this case 4) one option would be to loop over each track and pass them to the encoder one-by-one. However, this will create an unnecessary bottleneck. Instead, we use a small trick to compute all of the embeddings in the batch at once. \n",
    "\n",
    "We do this simply by moving all of the input recordings into the batch dimension. This will give us an effective batch size of `eff_bs = bs * num_tracks`. After moving all the recordings to the batch dimension, we can then pass them to the encoder, which expects a tensor of shape `(bs, seq_len)`. After generating these embeddings `e` it is simply a matter of reshaping the tensor so we can restore each embedding from the respective mix to the original dimension, which gives us a tensor of shape `(bs, num_tracks, d_embed)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec794e55",
   "metadata": {
    "id": "ec794e55"
   },
   "outputs": [],
   "source": [
    "# move tracks to the batch dimension to fully parallelize embedding computation\n",
    "x = x.view(bs * num_tracks, -1)\n",
    "print(f\"We get {bs}x{num_tracks} items in first dim: {x.shape}\")\n",
    "\n",
    "# generate single embedding for each track\n",
    "z = encoder(x)\n",
    "z = z.view(bs, num_tracks, -1)  # (bs, num_tracks, d_embed)\n",
    "print(f\"We get {num_tracks} embeddings of size {encoder.d_embed}: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf41ad",
   "metadata": {
    "id": "11bf41ad"
   },
   "source": [
    "### \"Context\" embedding\n",
    "\n",
    "<img width= \"40%\" src=\"https://csteinmetz1.github.io/automix-toolkit/docs/assets/dmc-context.svg\"/>\n",
    "\n",
    "Key to the DMC is the concept of the \"context\" emebdding which enables effective cross-channel communication between the recordings within a mixture when the post-processor will make a decision about the parameters for each channel. We compute the context embedding by simply taking the mean of all the track embeddings for each batch item. We can see this in the figure about represented as $z_\\mu$. After taking this mean we then copy (using `torch.repeat`) the mean embedding once for each track. This way we can then concatenate these copied embeddings with each of the track embeddings. \n",
    "\n",
    "However, recall that during training we use a fixed number of tracks and therefore some songs may have less than `num_tracks` *active* tracks where the other tracks are simply silence. These empty tracks will corrupt our context embedding. One way to handle this is to use the `track_mask` which is also provided by the dataset. This will be a tensor of boolean values telling us which tracks are not active, and show be masked. For example, consider the case where we have four total tracks but only the first three are active in the first batch item and all are active in the second. We would use the following `track_mask`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1Vv9pufA8fV",
   "metadata": {
    "id": "g1Vv9pufA8fV"
   },
   "outputs": [],
   "source": [
    "track_mask = torch.tensor(\n",
    "    [[False, False, False, True], \n",
    "     [False, False, False, False]]).view(2,-1)\n",
    "print(track_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdfd34",
   "metadata": {
    "id": "cbbdfd34"
   },
   "outputs": [],
   "source": [
    "# generate the \"context\" embedding\n",
    "c = []\n",
    "for bidx in range(bs): # loop over each batch for \"dynamic\" context computation\n",
    "    c_n = z[bidx, ~track_mask[bidx, :], :].mean(\n",
    "        dim=0, keepdim=True\n",
    "    )  # (bs, 1, d_embed)\n",
    "    c_n = c_n.repeat(num_tracks, 1)  # (bs, num_tracks, d_embed)\n",
    "    c.append(c_n)\n",
    "c = torch.stack(c, dim=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UTUDsN3nEHMO",
   "metadata": {
    "id": "UTUDsN3nEHMO"
   },
   "source": [
    "Note: Another way to implement this could be to fill the embeddings for non-active tracks with zeros and then take the sum across each batch item. Then to get the mean we could divide each sum by the number of `False` values in each `track_mask`. This would enable us to avoid the `for` loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RCppucPyA9B9",
   "metadata": {
    "id": "RCppucPyA9B9"
   },
   "source": [
    "At the end of this process we will have `num_tracks` embeddings each of size `d_embed*2` after the concatentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hzIiIB2hCTfe",
   "metadata": {
    "id": "hzIiIB2hCTfe"
   },
   "outputs": [],
   "source": [
    "# fuse the track embs and context embs\n",
    "z_final = torch.cat((z, c), dim=-1)  # (bs, num_tracks, d_embed*2)\n",
    "print(\"final embedding\", z_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bce8b",
   "metadata": {
    "id": "b55bce8b"
   },
   "source": [
    "### Estimate mixing parameters\n",
    "Now that we have the embeddings for ecah track we will use the Post-processor to estimate the mixing parameters (gain and panning) for each track. This will require running each of the `num_tracks` embeddings through the Post-Processor. However, the MLP class in PyTorch enables us to compute these in parallel automatically. So by passing our final embedding tensor of shape `(bs, num_tracks, d_embed*2)` into the Post-Processor we can generate all the mixing parameters. As we see below, we will get a parameter tensor containing 2 parameters (gain and pan) for each of the `num_tracks`. Almost there, now the final step is to use these parameters and the `Mixer` to create the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce9654",
   "metadata": {
    "id": "9cce9654"
   },
   "outputs": [],
   "source": [
    "# estimate mixing parameters for each track (in parallel)\n",
    "p = postprocessor(z_final)  # (bs, num_tracks, num_params)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99a276",
   "metadata": {
    "id": "4a99a276"
   },
   "source": [
    "### Generate the mix\n",
    "We already discussed how the `Mixer` is implemented. Here we will call the mixer passing in the tracks as well as the parameters we just predicted. Inside the `Mixer` these parameters will be denormalized from 0 to 1 to their full range. \n",
    "\n",
    "We can see that we get two return values from calling the `Mixer`. The first is the stereo mix, which is the same length as the inputs but has only two channels. We also get a new tensor for the parameters, which is the same shape. This tensor contains the parameter values, but in their denormalized state. This will enable us to inspect what parameters were estimated by the model in the human interpretable form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cd3fd",
   "metadata": {
    "id": "0f6cd3fd"
   },
   "outputs": [],
   "source": [
    "# generate the stereo mix\n",
    "x = x.view(bs, num_tracks, -1)  # move tracks back from batch dim\n",
    "y, p = mixer(x, p)  # (bs, 2, seq_len) # and denormalized params\n",
    "print(y.shape, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06744b1e",
   "metadata": {
    "id": "06744b1e"
   },
   "source": [
    "We can easily print out the parameters for each track as follows. In this example all of the parameters are very similar since we are using an untrained network and noise as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabf8aa",
   "metadata": {
    "id": "4eabf8aa"
   },
   "outputs": [],
   "source": [
    "for tidx, track_params in enumerate(p[0,...]):\n",
    "    print(f\"{tidx} gain dB:{track_params[0]:0.3f}  pan:{track_params[1]:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7b6cd",
   "metadata": {
    "id": "d8c7b6cd"
   },
   "source": [
    "That concludes the section on the models. Hopefully this provided some insight into the innerworkings of these two automatic mixing models. Both implementations are simple and could be built on to extend their features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92b983-0ccb-46b5-bdf8-f31b6eb0d31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba271cd-acd7-4699-9899-2d2b166e4464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e262d-f34e-4678-9476-dade74030dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a13298-32ed-458a-ad86-07c30322c8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
